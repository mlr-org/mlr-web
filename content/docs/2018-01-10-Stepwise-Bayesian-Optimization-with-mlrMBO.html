---
title: "Stepwise Bayesian Optimization with mlrMBO"
authors: ["jakob-richter"]
date: 2018-01-10
categories: ["R", "r-bloggers"]
tags: ["mlrMBO", "Bayesian", "optimization", "tuning", "stepwise"]
---



<p>With the release of the new version of <a href="https://mlr-org.github.io/mlrMBO/">mlrMBO</a> we added some minor fixes and added a practical feature called <em><a href="https://mlr-org.github.io/mlrMBO/articles/supplementary/human_in_the_loop_MBO.html">Human-in-the-loop MBO</a></em>.
It enables you to sequentially</p>
<ul>
<li>visualize the state of the surrogate model,</li>
<li>obtain the suggested parameter configuration for the next iteration and</li>
<li>update the surrogate model with arbitrary evaluations.</li>
</ul>
<p>In the following we will demonstrate this feature on a simple example.</p>
<p>First we need an objective function we want to optimize.
For this post a simple function will suffice but note that this function could also be an external process as in this mode <strong>mlrMBO</strong> does not need to access the objective function as you will only have to pass the results of the function to <strong>mlrMBO</strong>.</p>
<pre class="r"><code>library(mlrMBO)
library(ggplot2)
set.seed(1)

fun = function(x) {
  x^2 + sin(2 * pi * x) * cos(0.3 * pi * x)
}</code></pre>
<p>However we still need to define the our search space.
In this case we look for a real valued value between -3 and 3.
For more hints about how to define ParamSets you can look <a href="http://jakob-r.de/mlrHyperopt/articles/working_with_parconfigs_and_paramsets.html#the-basics-of-a-paramset">here</a> or in the <a href="https://rdrr.io/cran/ParamHelpers/man/makeParamSet.html">help of ParamHelpers</a>.</p>
<pre class="r"><code>ps = makeParamSet(
  makeNumericParam(&quot;x&quot;, lower = -3, upper = 3)
)</code></pre>
<p>We also need some initial evaluations to start the optimization.
The design has to be passed as a <code>data.frame</code> with one column for each dimension of the search space and one column <code>y</code> for the outcomes of the objective function.</p>
<pre class="r"><code>des = generateDesign(n = 3, par.set = ps)
des$y = apply(des, 1, fun)
des
##            x         y
## 1 -1.1835844 0.9988801
## 2 -0.5966361 0.8386779
## 3  2.7967794 8.6592973</code></pre>
<p>With these values we can initialize our sequential MBO object.</p>
<pre class="r"><code>ctrl = makeMBOControl()
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
opt.state = initSMBO(
  par.set = ps, 
  design = des, 
  control = ctrl, 
  minimize = TRUE, 
  noisy = FALSE)</code></pre>
<p>The <code>opt.state</code> now contains all necessary information for the optimization.
We can even plot it to see how the Gaussian process models the objective function.</p>
<pre class="r"><code>plot(opt.state)</code></pre>
<p><img src="/https:/mlr-web.netlify.com/docs/2018-01-10-Stepwise-Bayesian-Optimization-with-mlrMBO_files/figure-html/optstate1-1.png" width="672" /></p>
<p>In the first panel the <em>expected improvement</em> (<span class="math inline">\(EI = E(y_{min}-\hat{y})\)</span>) (see <a href="http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf">Jones et.al.</a>) is plotted over the search space.
The maximum of the <em>EI</em> indicates the point that we should evaluate next.
The second panel shows the mean prediction of the surrogate model, which is the Gaussian regression model aka <em>Kriging</em> in this example.
The third panel shows the uncertainty prediction of the surrogate.
We can see, that the <em>EI</em> is high at points, where the mean prediction is low and/or the uncertainty is high.</p>
<p>To obtain the specific configuration suggested by mlrMBO for the next evaluation of the objective we can run:</p>
<pre class="r"><code>prop = proposePoints(opt.state)
prop
## $prop.points
##             x
## 969 -2.999979
## 
## $propose.time
## [1] 0.038
## 
## $prop.type
## [1] &quot;infill_ei&quot;
## 
## $crit.vals
##            [,1]
## [1,] -0.3733677
## 
## $crit.components
##       se     mean
## 1 2.8899 3.031364
## 
## $errors.model
## [1] NA
## 
## attr(,&quot;class&quot;)
## [1] &quot;Proposal&quot; &quot;list&quot;</code></pre>
<p>We will execute our objective function with the suggested value for <code>x</code> and feed it back to mlrMBO:</p>
<pre class="r"><code>y = fun(prop$prop.points$x)
y
## [1] 8.999752
updateSMBO(opt.state, x = prop$prop.points, y = y)</code></pre>
<p>The nice thing about the <em>human-in-the-loop</em> mode is, that you don’t have to stick to the suggestion.
In other words we can feed the model with values without receiving a proposal.
Let’s assume we have an expert who tells us to evaluate the values <span class="math inline">\(x=-1\)</span> and <span class="math inline">\(x=1\)</span> we can easily do so:</p>
<pre class="r"><code>custom.prop = data.frame(x = c(-1,1))
ys = apply(custom.prop, 1, fun)
updateSMBO(opt.state, x = custom.prop, y = as.list(ys))
plot(opt.state, scale.panels = TRUE)</code></pre>
<p><img src="/https:/mlr-web.netlify.com/docs/2018-01-10-Stepwise-Bayesian-Optimization-with-mlrMBO_files/figure-html/feedmanual-1.png" width="672" /></p>
<p>We can also automate the process easily:</p>
<pre class="r"><code>replicate(3, {
  prop = proposePoints(opt.state)
  y = fun(prop$prop.points$x)
  updateSMBO(opt.state, x = prop$prop.points, y = y)
})</code></pre>
<p><em>Note:</em> We suggest to use the normal mlrMBO if you are only doing this as mlrMBO has more advanced logging, termination and handling of errors etc.</p>
<p>Let’s see how the surrogate models the true objective function after having seen seven configurations:</p>
<pre class="r"><code>plot(opt.state, scale.panels = TRUE)</code></pre>
<p><img src="/https:/mlr-web.netlify.com/docs/2018-01-10-Stepwise-Bayesian-Optimization-with-mlrMBO_files/figure-html/optstate2-1.png" width="672" /></p>
<p>You can convert the <code>opt.state</code> object from this run to a normal mlrMBO result object like this:</p>
<pre class="r"><code>res = finalizeSMBO(opt.state)
res
## Recommended parameters:
## x=-0.22
## Objective: y = -0.913
## 
## Optimization path
## 3 + 6 entries in total, displaying last 10 (or less):
##            x          y dob eol error.message exec.time         ei
## 1 -1.1835844  0.9988801   0  NA          &lt;NA&gt;        NA         NA
## 2 -0.5966361  0.8386779   0  NA          &lt;NA&gt;        NA         NA
## 3  2.7967794  8.6592973   0  NA          &lt;NA&gt;        NA         NA
## 4 -2.9999793  8.9997519   4  NA          &lt;NA&gt;        NA -0.3733677
## 5 -1.0000000  1.0000000   5  NA          &lt;NA&gt;        NA -0.3136111
## 6  1.0000000  1.0000000   6  NA          &lt;NA&gt;        NA -0.1366630
## 7  0.3010828  1.0016337   7  NA          &lt;NA&gt;        NA -0.7750916
## 8 -0.2197165 -0.9126980   8  NA          &lt;NA&gt;        NA -0.1569065
## 9 -0.1090728 -0.6176863   9  NA          &lt;NA&gt;        NA -0.1064289
##   error.model train.time  prop.type propose.time        se       mean
## 1        &lt;NA&gt;         NA initdesign           NA        NA         NA
## 2        &lt;NA&gt;         NA initdesign           NA        NA         NA
## 3        &lt;NA&gt;         NA initdesign           NA        NA         NA
## 4        &lt;NA&gt;          0     manual           NA 2.8899005  3.0313640
## 5        &lt;NA&gt;          0     manual           NA 0.5709559  0.6836938
## 6        &lt;NA&gt;         NA       &lt;NA&gt;           NA 3.3577897  5.3791930
## 7        &lt;NA&gt;          0     manual           NA 1.2337881  0.3493416
## 8        &lt;NA&gt;          0     manual           NA 0.4513106  0.8870228
## 9        &lt;NA&gt;          0     manual           NA 0.3621550 -0.8288961</code></pre>
<p><em>Note:</em> You can always run the <em>human-in-the-loop MBO</em> on <code>res$final.opt.state</code>.</p>
<p>For the curious, let’s see how our original function actually looks like and which points we evaluated during our optimization:</p>
<pre class="r"><code>plot(fun, -3, 3)
points(x = getOptPathX(res$opt.path)$x, y = getOptPathY(res$opt.path))</code></pre>
<p><img src="/https:/mlr-web.netlify.com/docs/2018-01-10-Stepwise-Bayesian-Optimization-with-mlrMBO_files/figure-html/plottrue-1.png" width="672" /></p>
<p>We can see, that we got pretty close to the global optimum and that the surrogate in the previous plot models the objective quite accurate.</p>
<p>For more in-depth information look at the <a href="https://mlr-org.github.io/mlrMBO/articles/supplementary/human_in_the_loop_MBO.html">Vignette for Human-in-the-loop MBO</a> and check out the other topics of our <a href="https://mlr-org.github.io/mlrMBO">mlrMBO page</a>.</p>
