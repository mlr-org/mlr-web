---
title: "Exploring Learner Predictions with Partial Dependence and Functional ANOVA"
authors: ["zachary-jones"]
date: 2016-08-11
draft: true
categories: ["R", "r-bloggers"]
tags: ["partial dependence", "benchmark", "prediction", "ANOVA", "rstats"]
---



<p>Learners use features to make predictions but how those features are used is often not apparent.
<a href="http://github.com/mlr-org/mlr">mlr</a> can estimate the dependence of a learned function on a subset of the feature space using
<code>generatePartialDependenceData</code>.</p>
<p>Partial dependence plots reduce the potentially high dimensional function estimated by the learner, and display a marginalized version of this function in a lower dimensional space. For example suppose <span class="math inline">\(\mathbb{E}[Y \ \| \ X = x] = f(x)\)</span>. With <span class="math inline">\((x, y)\)</span> pairs drawn independently, a learner may estimate <span class="math inline">\(\hat{f}\)</span>, which, if <span class="math inline">\(X\)</span> is high dimensional can be uninterpretable. Suppose we want to approximate the relationship between some column-wise subset of <span class="math inline">\(X\)</span>. We partition <span class="math inline">\(X\)</span> into two sets, <span class="math inline">\(X_s\)</span> and <span class="math inline">\(X_c\)</span> such that <span class="math inline">\(X = X_s \cup X_c\)</span>, where <span class="math inline">\(X_s\)</span> is a subset of <span class="math inline">\(X\)</span> of interest.</p>
<p>The partial dependence of <span class="math inline">\(f\)</span> on <span class="math inline">\(X_c\)</span> is</p>
<p><span class="math display">\[f_{X_s} = \mathbb{E}_{X_c}f(X_s, X_c).\]</span></p>
<p>We can use the following estimator:</p>
<p><span class="math display">\[\hat{f}_{x_s} = \frac{1}{N} \sum_{i = 1}^N \hat{f}(x_s, x_{ic}).\]</span></p>
<p>This is described by <a href="https://projecteuclid.org/euclid.aos/1013203451">Friedman (2001)</a> and in <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Hastie, Tibsharani, and Friedman (2009)</a>.</p>
<p>The individual conditional expectation of an observation can also be estimated using the above algorithm absent the averaging, giving <span class="math inline">\(\hat{f}^{(i)}_{x_s}\)</span> as described in <a href="https://arxiv.org/abs/1309.6392">Goldstein, Kapelner, Bleich, and Pitkin (2014)</a>. This allows the discovery of features of <span class="math inline">\(\hat{f}\)</span> that may be obscured by an aggregated summary of <span class="math inline">\(\hat{f}\)</span>.</p>
<p>The partial derivative of the partial dependence function, <span class="math inline">\(\frac{\partial \hat f_{x_s}}{\partial x_s}\)</span>, and the individual conditional expectation function, <span class="math inline">\(\frac{\partial \hat{f}^{(i)}_{x_s}}{\partial x_s}\)</span>, can also be computed. For regression and survival tasks the partial derivative of a single feature <span class="math inline">\(x_s\)</span> is the gradient of the partial dependence function, and for classification tasks where the learner can output class probabilities the Jacobian. Note that if the learner produces discontinuous partial dependence (e.g., piecewise constant functions such as decision trees, ensembles of decision trees, etc.) the derivative will be 0 (where the function is not changing) or trending towards positive or negative infinity (at the discontinuities where the derivative is undefined). Plotting the partial dependence function of such learners may give the impression that the function is not discontinuous because the prediction grid is not composed of all discontinuous points in the predictor space. This results in a line interpolating that makes the function appear to be piecewise linear (where the derivative would be defined except at the boundaries of each piece).</p>
<p>The partial derivative can be informative regarding the additivity of the learned function in certain features. If <span class="math inline">\(\hat{f}^{(i)}_{x_s}\)</span> is an additive function in a feature <span class="math inline">\(x_s\)</span>, then its partial derivative will not depend on any other features (<span class="math inline">\(x_c\)</span>) that may have been used by the learner. Variation in the estimated partial derivative indicates that there is a region of interaction between <span class="math inline">\(x_s\)</span> and <span class="math inline">\(x_c\)</span> in <span class="math inline">\(\hat{f}\)</span>. Similarly, instead of using the mean to estimate the expected value of the function at different values of <span class="math inline">\(x_s\)</span>, instead computing the variance can highlight regions of interaction between <span class="math inline">\(x_s\)</span> and <span class="math inline">\(x_c\)</span>.</p>
<p>Again, see <a href="http://arxiv.org/abs/1309.6392">Goldstein, Kapelner, Bleich, and Pitkin (2014)</a> for more details and their package <a href="https://cran.r-project.org/web/packages/ICEbox/index.html">ICEbox</a> for the original implementation. The algorithm works for any supervised learner with classification, regression, and survival tasks.</p>
<div id="partial-dependence" class="section level2">
<h2>Partial Dependence</h2>
<p>Our implementation, following <a href="http://github.com/mlr-org/mlr">mlr</a>’s <a href="https://mlr.mlr-org.com/articles/tutorial/visualization.html">visualization</a> pattern, consists
of the above mentioned function <code>generatePartialDependenceData</code> and <code>plotPartialDependence</code>. The former generates input (objects of class <code>PartialDependenceData</code>) for the latter.</p>
<p>The first step executed by <code>generatePartialDependenceData</code> is to generate a feature grid for every element of the character vector <code>features</code> passed. The data are given by the <code>input</code> argument, which can be a <code>Task</code> or a <code>data.frame</code>. The feature grid can be generated in several ways. A uniformly spaced grid of length <code>gridsize</code> (default 10) from the empirical minimum to the empirical maximum is created by default, but arguments <code>fmin</code> and <code>fmax</code> may be used to override the empirical default (the lengths of <code>fmin</code> and <code>fmax</code> must match the length of <code>features</code>). Alternatively the feature data can be resampled, either by using a bootstrap or by subsampling.</p>
<p>Results from <code>generatePartialDependenceData</code> can be visualized with <code>plotPartialDependence</code>.</p>
<pre class="r"><code>library(mlr)

lrn.classif = makeLearner(&quot;classif.ksvm&quot;, predict.type = &quot;prob&quot;)
fit.classif = train(lrn.classif, iris.task)
pd = generatePartialDependenceData(fit.classif, iris.task, &quot;Petal.Width&quot;)
pd

plotPartialDependence(pd, data = iris)</code></pre>
<p>As noted above, <span class="math inline">\(x_s\)</span> does not have to be unidimensional. If it is not, the <code>interaction</code> flag must be set to <code>TRUE</code>. Then the individual feature grids are combined using the Cartesian product, and the estimator above is applied, producing the partial dependence for every combination of unique feature values. If the <code>interaction</code> flag is <code>FALSE</code> (the default) then by default <span class="math inline">\(x_s\)</span> is assumed unidimensional, and partial dependencies are generated for each feature separately. The resulting output when <code>interaction = FALSE</code> has a column for each feature, and <code>NA</code> where the feature was not used. With one feature and a regression task the output is a line plot, with a point for each point in the corresponding feature’s grid. For classification tasks there is a line for each class (except for binary classification tasks, where the negative class is automatically dropped). The <code>data</code> argument to <code>plotPartialPrediction</code> allows the training data to be input to show the empirical marginal distribution of the data.</p>
<pre class="r"><code>pd.lst = generatePartialDependenceData(fit.classif, iris.task, c(&quot;Petal.Width&quot;, &quot;Petal.Length&quot;))
head(pd.lst$data)

tail(pd.lst$data)

plotPartialDependence(pd.lst, data = iris)</code></pre>
<pre class="r"><code>pd.int = generatePartialDependenceData(fit.classif, iris.task, c(&quot;Petal.Width&quot;, &quot;Petal.Length&quot;), interaction = TRUE)
pd.int

plotPartialDependence(pd.int, facet = &quot;Petal.Length&quot;)</code></pre>
<p>When <code>interaction = TRUE</code>, <code>plotPartialDependence</code> can either facet over one feature, showing the conditional relationship between the other feature and <span class="math inline">\(\hat{f}\)</span> in each panel, or a tile plot. The latter is, however, not possible with multiclass classification (an example of a tile plot will be shown later).</p>
<p>At each step in the estimation of <span class="math inline">\(\hat{f}_{x_s}\)</span> a set of predictions of length <span class="math inline">\(N\)</span> is generated. By default the mean prediction is used. For classification where <code>predict.type = &quot;prob&quot;</code> this entails the mean class probabilities. However, other summaries of the predictions may be used. For regression and survival tasks the function used here must either return one number or three, and, if the latter, the numbers must be sorted lowest to highest. For classification tasks the function must return a number for each level of the target feature.</p>
<p>As noted, the <code>fun</code> argument can be a function which returns three numbers (sorted low to high) for a regression task. This allows further exploration of relative feature importance. If a feature is relatively important, the bounds are necessarily tighter because the feature accounts for more of the variance of the predictions, i.e., it is “used” more by the learner. More directly setting <code>fun = var</code> identifies regions of interaction between <span class="math inline">\(x_s\)</span> and <span class="math inline">\(x_c\)</span>. This can also be accomplished by computing quantiles. The wider the quantile bounds, the more variation in <span class="math inline">\(\hat{f}\)</span> is due to features other than <span class="math inline">\(x_s\)</span> that is shown in the plot.</p>
<pre class="r"><code>lrn.regr = makeLearner(&quot;regr.ksvm&quot;)
fit.regr = train(lrn.regr, bh.task)

pd.ci = generatePartialDependenceData(fit.regr, bh.task, &quot;lstat&quot;,
  fun = function(x) quantile(x, c(.25, .5, .75)))
pd.ci

plotPartialDependence(pd.ci)</code></pre>
<p>In addition to bounds based on a summary of the distribution of the conditional expectation of each observation, learners which can estimate the variance of their predictions can also be used. The argument <code>bounds</code> is a numeric vector of length two which is added (so the first number should be negative) to the point prediction to produce a confidence interval for the partial dependence. The default is the .025 and .975 quantiles of the Gaussian distribution.</p>
<pre class="r"><code>fit.se = train(makeLearner(&quot;regr.randomForest&quot;, predict.type = &quot;se&quot;), bh.task)
pd.se = generatePartialDependenceData(fit.se, bh.task, c(&quot;lstat&quot;, &quot;crim&quot;))
head(pd.se$data)

tail(pd.se$data)

plotPartialDependence(pd.se)</code></pre>
<p>As previously mentioned if the aggregation function is not used, i.e., it is the identity, then the conditional expectation of <span class="math inline">\(\hat{f}^{(i)}_{x_s}\)</span> is estimated. If <code>individual = TRUE</code> then <code>generatePartialDependenceData</code> returns <span class="math inline">\(N\)</span> partial dependence estimates made at each point in the prediction grid constructed from the features.</p>
<pre class="r"><code>pd.ind.regr = generatePartialDependenceData(fit.regr, bh.task, &quot;lstat&quot;, individual = TRUE)
pd.ind.regr

plotPartialDependence(pd.ind.regr)</code></pre>
<p>The resulting output, particularly the element <code>data</code> in the returned object, has an additional column <code>idx</code> which gives the index of the observation to which the row pertains.</p>
<p>For classification tasks this index references both the class and the observation index.</p>
<pre class="r"><code>pd.ind.classif = generatePartialDependenceData(fit.classif, iris.task, &quot;Petal.Length&quot;, individual = TRUE)
pd.ind.classif
plotPartialDependence(pd.ind.classif)</code></pre>
<p>The plots, at least in these forms, are difficult to interpet. Individual estimates of partial dependence can also be centered by predictions made at all <span class="math inline">\(N\)</span> observations
for a particular point in the prediction grid created by the features. This is controlled by the argument <code>center</code> which is a list of the same length as the length of the <code>features</code> argument and contains the values of the <code>features</code> desired.</p>
<pre class="r"><code>pd.ind.classif = generatePartialDependenceData(fit.classif, iris.task, &quot;Petal.Length&quot;, individual = TRUE, center = list(&quot;Petal.Length&quot; = min(iris$Petal.Length)))
plotPartialDependence(pd.ind.classif)</code></pre>
<p>Partial derivatives can also be computed for individual partial dependence estimates and aggregate partial dependence. This is restricted to a single feature at a time. The derivatives of individual partial dependence estimates can be useful in finding regions of interaction between the feature for which the derivative is estimated and the features excluded. Applied to the aggregated partial dependence function they are not very informative, but when applied to the individual conditional expectations, they can be used to find regions of interaction.</p>
<pre class="r"><code>pd.regr.der.ind = generatePartialDependenceData(fit.regr, bh.task, &quot;lstat&quot;, derivative = TRUE, individual = TRUE)
head(pd.regr.der.ind$data)

plotPartialDependence(pd.regr.der.ind)</code></pre>
<pre class="r"><code>pd.classif.der.ind = generatePartialDependenceData(fit.classif, iris.task, &quot;Petal.Width&quot;, derivative = TRUE, individual = TRUE)
head(pd.classif.der.ind$data)

plotPartialDependence(pd.classif.der.ind)</code></pre>
<p>This suggests that <code>Petal.Width</code> interacts with some other feature in the neighborhood of <span class="math inline">\((1.5, 2)\)</span> for classes “virginica” and “versicolor”.</p>
</div>
<div id="functional-anova" class="section level2">
<h2>Functional ANOVA</h2>
<p><a href="http://dl.acm.org/citation.cfm?id=1014122">Hooker (2004)</a> proposed the decomposition of a learned function <span class="math inline">\(\hat{f}\)</span> as a sum of lower dimensional functions <span class="math display">\[f(\mathbf{x}) = g_0 + \sum_{i = 1}^p g_{i}(x_i) + \sum_{i \neq j} g_{ij}(x_{ij}) + \ldots\]</span> where <span class="math inline">\(p\)</span> is the number of features. <code>generateFunctionalANOVAData</code> estimates the individual <span class="math inline">\(g\)</span> functions using partial dependence. When functions depend only on one feature, they are equivalent to partial dependence, but a <span class="math inline">\(g\)</span> function which depends on more than one feature is the “effect” of only those features: lower dimensional “effects” are removed.</p>
<p><span class="math display">\[\hat{g}_u(x) = \frac{1}{N} \sum_{i = 1}^N \left( \hat{f}(x) - \sum_{v \subset u} g_v(x) \right)\]</span></p>
<p>Here <span class="math inline">\(u\)</span> is a subset of <span class="math inline">\({1, \ldots, p}\)</span>. When <span class="math inline">\(\|v\| = 1\)</span> <span class="math inline">\(g_v\)</span> can be directly computed by computing the bivariate partial dependence of <span class="math inline">\(\hat{f}\)</span> on <span class="math inline">\(x_u\)</span> and then subtracting off the univariate partial dependences of the features contained in <span class="math inline">\(v\)</span>.</p>
<p>Although this decomposition is generalizable to classification it is currently only available for regression tasks.</p>
<pre class="r"><code>lrn.regr = makeLearner(&quot;regr.ksvm&quot;)
fit.regr = train(lrn.regr, bh.task)

fa = generateFunctionalANOVAData(fit.regr, bh.task, &quot;lstat&quot;, depth = 1, fun = median)
fa

pd.regr = generatePartialDependenceData(fit.regr, bh.task, &quot;lstat&quot;, fun = median)
pd.regr</code></pre>
<p>The <code>depth</code> argument is similar to the <code>interaction</code> argument in <code>generatePartialDependenceData</code> but instead of specifying whether all of joint “effect” of all the <code>features</code> is computed, it determines whether “effects” of all subsets of the features given the specified <code>depth</code> are computed. So, for example, with <span class="math inline">\(p\)</span> features and depth 1, the univariate partial dependence is returned. If, instead, <code>depth = 2</code>, then all possible bivariate functional ANOVA effects are returned. This is done by computing the univariate partial dependence for each feature and subtracting it from the bivariate partial dependence for each possible pair.</p>
<pre class="r"><code>fa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c(&quot;crim&quot;, &quot;lstat&quot;, &quot;age&quot;), depth = 2)
fa.bv

names(table(fa.bv$data$effect)) ## interaction effects estimated</code></pre>
<p>Plotting univariate and bivariate functional ANOVA components works the same as for partial dependence.</p>
<pre class="r"><code>fa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c(&quot;crim&quot;, &quot;lstat&quot;), depth = 2)
plotPartialDependence(fa.bv, geom = &quot;tile&quot;, data = getTaskData(bh.task))</code></pre>
<p>When overplotting the training data on the plot it is easy to see that much of the variation of the effect is due to extrapolation. Although it hasn’t been implemented yet, weighting the functional ANOVA appropriately can ensure that the estimated effects do not depend (or depend less) on regions of the feature space which are sparse.</p>
<p>I also plan on implementing the faster estimation algorith for expanding the functionality of the functional ANOVA function include faster computation using the algorithm in <a href="http://faculty.bscb.cornell.edu/~hooker/fame_jcgs.pdf">Hooker (2007)</a> and weighting (in order to avoid excessive reliance on points of extrapolation) using outlier detection or joint density estimation.</p>
</div>
