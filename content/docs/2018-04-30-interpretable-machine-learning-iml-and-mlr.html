---
title: "Interpretable Machine Learning with iml and mlr"
authors: ["christoph-molnar"]
date: 2018-02-28
categories: ["R"]
# tags: ["R Markdown", "plot", "regression"]
---



<p>Machine learning models repeatedly outperform interpretable, parametric models like the linear regression model.
The gains in performance have a price: The models operate as black boxes which are not interpretable.</p>
<p>Fortunately, there are many methods that can make machine learning models interpretable.
The R package <code>iml</code> provides tools for analysing any black box machine learning model:</p>
<ul>
<li>Feature importance: Which were the most important features?</li>
<li>Feature effects: How does a feature influence the prediction? (Partial dependence plots and individual conditional expectation curves)</li>
<li>Explanations for single predictions: How did the feature values of a single data point affect its prediction? (LIME and Shapley value)</li>
<li>Surrogate trees: Can we approximate the underlying black box model with a short decision tree?</li>
<li>The iml package works for any classification and regression machine learning model: random forests, linear models, neural networks, xgboost, etc.</li>
</ul>
<p>This blog post shows you how to use the <code>iml</code> package to analyse machine learning models.
While the <code>mlr</code> package makes it super easy to train machine learning models, the <code>iml</code> package makes it easy to extract insights about the learned black box machine learning models.</p>
<p>If you want to learn more about the technical details of all the methods, read the <a href="https://christophm.github.io/interpretable-ml-book/agnostic.html">Interpretable Machine Learning book</a>.</p>
<p><img src="/images/2018-04-30-interpretable-machine-learning-iml-and-mlr/iml-bear.jpg" /></p>
<p>Let’s explore the <code>iml</code>-toolbox for interpreting an <code>mlr</code> machine learning model with concrete examples!</p>
<div id="data-boston-housing" class="section level2">
<h2>Data: Boston Housing</h2>
<p>We’ll use the <code>MASS::Boston</code> dataset to demonstrate the abilities of the iml package. This dataset contains median house values from Boston neighbourhoods.</p>
<pre class="r"><code>data(&quot;Boston&quot;, package  = &quot;MASS&quot;)
head(Boston)
#&gt;      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black
#&gt; 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90
#&gt; 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90
#&gt; 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83
#&gt; 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63
#&gt; 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90
#&gt; 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12
#&gt;   lstat medv
#&gt; 1  4.98 24.0
#&gt; 2  9.14 21.6
#&gt; 3  4.03 34.7
#&gt; 4  2.94 33.4
#&gt; 5  5.33 36.2
#&gt; 6  5.21 28.7</code></pre>
</div>
<div id="fitting-the-machine-learning-model" class="section level2">
<h2>Fitting the machine learning model</h2>
<p>First we train a randomForest to predict the Boston median housing value:</p>
<pre class="r"><code>library(&quot;mlr&quot;)
data(&quot;Boston&quot;, package  = &quot;MASS&quot;)

# create an mlr task and model
tsk = makeRegrTask(data = Boston, target = &quot;medv&quot;)
lrn = makeLearner(&quot;regr.randomForest&quot;, ntree = 100)
mod = train(lrn, tsk)</code></pre>
</div>
<div id="using-the-iml-predictor-container" class="section level2">
<h2>Using the iml Predictor container</h2>
<p>We create a <code>Predictor</code> object, that holds the model and the data. The <code>iml</code> package uses R6 classes: New objects can be created by calling <code>Predictor$new()</code>.
<code>Predictor</code> works best with mlr models (<code>WrappedModel</code>-class), but it is also possible to use models from other packages.</p>
<pre class="r"><code>library(&quot;iml&quot;)
X = Boston[which(names(Boston) != &quot;medv&quot;)]
predictor = Predictor$new(mod, data = X, y = Boston$medv)</code></pre>
</div>
<div id="feature-importance" class="section level2">
<h2>Feature importance</h2>
<p>We can measure how important each feature was for the predictions with <code>FeatureImp</code>. The feature importance measure works by shuffling each feature and measuring how much the performance drops. For this regression task we choose to measure the loss in performance with the mean absolute error (‘mae’); another choice would be the mean squared error (‘mse’).</p>
<p>Once we created a new object of <code>FeatureImp</code>, the importance is automatically computed.
We can call the <code>plot()</code> function of the object or look at the results in a data.frame.</p>
<pre class="r"><code>imp = FeatureImp$new(predictor, loss = &quot;mae&quot;)
plot(imp)</code></pre>
<p><img src="/https:/mlr-web.netlify.comdocs/2018-04-30-interpretable-machine-learning-iml-and-mlr_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>imp$results
#&gt;    feature original.error permutation.error importance
#&gt; 1    lstat      0.9546924         4.4301626   4.640408
#&gt; 2       rm      0.9546924         3.4838984   3.649237
#&gt; 3      nox      0.9546924         1.6936588   1.774036
#&gt; 4     crim      0.9546924         1.6168185   1.693549
#&gt; 5      dis      0.9546924         1.5895507   1.664987
#&gt; 6  ptratio      0.9546924         1.5544831   1.628255
#&gt; 7    indus      0.9546924         1.5182089   1.590260
#&gt; 8      age      0.9546924         1.4034536   1.470058
#&gt; 9      tax      0.9546924         1.3047505   1.366671
#&gt; 10   black      0.9546924         1.2429762   1.301965
#&gt; 11     rad      0.9546924         1.0200721   1.068483
#&gt; 12    chas      0.9546924         0.9989070   1.046313
#&gt; 13      zn      0.9546924         0.9587879   1.004290</code></pre>
</div>
<div id="partial-dependence" class="section level2">
<h2>Partial dependence</h2>
<p>Besides learning which features were important, we are interested in how the features influence the predicted outcome. The <code>Partial</code> class implements partial dependence plots and individual conditional expectation curves. Each individual line represents the predictions (y-axis) for one data point when we change one of the features (e.g. ‘lstat’ on the x-axis). The highlighted line is the point-wise average of the individual lines and equals the partial dependence plot. The marks on the x-axis indicates the distribution of the ‘lstat’ feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).</p>
<pre class="r"><code>pdp.obj = Partial$new(predictor, feature = &quot;lstat&quot;)
plot(pdp.obj)</code></pre>
<p><img src="/https:/mlr-web.netlify.comdocs/2018-04-30-interpretable-machine-learning-iml-and-mlr_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>If we want to compute the partial dependence curves for another feature, we can simply reset the feature.
Also, we can center the curves at a feature value of our choice, which makes it easier to see the trend of the curves:</p>
<pre class="r"><code>pdp.obj$set.feature(&quot;rm&quot;)
pdp.obj$center(min(Boston$rm))
plot(pdp.obj)</code></pre>
<p><img src="/https:/mlr-web.netlify.comdocs/2018-04-30-interpretable-machine-learning-iml-and-mlr_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="surrogate-model" class="section level2">
<h2>Surrogate model</h2>
<p>Another way to make the models more interpretable is to replace the black box with a simpler model - a decision tree. We take the predictions of the black box model (in our case the random forest) and train a decision tree on the original features and the predicted outcome.
The plot shows the terminal nodes of the fitted tree.
The maxdepth parameter controls how deep the tree can grow and therefore how interpretable it is.</p>
<pre class="r"><code>tree = TreeSurrogate$new(predictor, maxdepth = 2)
plot(tree)</code></pre>
<p><img src="/https:/mlr-web.netlify.comdocs/2018-04-30-interpretable-machine-learning-iml-and-mlr_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We can use the tree to make predictions:</p>
<pre class="r"><code>head(tree$predict(Boston))
#&gt;     .y.hat
#&gt; 1 28.36981
#&gt; 2 21.76483
#&gt; 3 28.36981
#&gt; 4 28.36981
#&gt; 5 28.36981
#&gt; 6 28.36981</code></pre>
</div>
<div id="explain-single-predictions-with-a-local-model" class="section level2">
<h2>Explain single predictions with a local model</h2>
<p>Global surrogate model can improve the understanding of the global model behaviour.
We can also fit a model locally to understand an individual prediction better. The local model fitted by <code>LocalModel</code> is a linear regression model and the data points are weighted by how close they are to the data point for wich we want to explain the prediction.</p>
<pre class="r"><code>lime.explain = LocalModel$new(predictor, x.interest = X[1,])
lime.explain$results
#&gt;               beta x.recoded    effect x.original feature feature.value
#&gt; rm       4.3059097     6.575 28.311356      6.575      rm      rm=6.575
#&gt; ptratio -0.5208891    15.300 -7.969603       15.3 ptratio  ptratio=15.3
#&gt; lstat   -0.4295847     4.980 -2.139332       4.98   lstat    lstat=4.98
plot(lime.explain)</code></pre>
<p><img src="/https:/mlr-web.netlify.comdocs/2018-04-30-interpretable-machine-learning-iml-and-mlr_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="explain-single-predictions-with-game-theory" class="section level2">
<h2>Explain single predictions with game theory</h2>
<p>An alternative for explaining individual predictions is a method from coalitional game theory named Shapley value.
Assume that for one data point, the feature values play a game together, in which they get the prediction as a payout. The Shapley value tells us how to fairly distribute the payout among the feature values.</p>
<pre class="r"><code>shapley = Shapley$new(predictor, x.interest = X[1,])
plot(shapley)</code></pre>
<p><img src="/https:/mlr-web.netlify.comdocs/2018-04-30-interpretable-machine-learning-iml-and-mlr_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We can reuse the object to explain other data points:</p>
<pre class="r"><code>shapley$explain(x.interest = X[2,])
plot(shapley)</code></pre>
<p><img src="/https:/mlr-web.netlify.comdocs/2018-04-30-interpretable-machine-learning-iml-and-mlr_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The results in data.frame form can be extracted like this:</p>
<pre class="r"><code>results = shapley$results
head(results)
#&gt;   feature         phi      phi.var feature.value
#&gt; 1    crim  0.00678256  1.084280662  crim=0.02731
#&gt; 2      zn -0.03610350  0.007022100          zn=0
#&gt; 3   indus  0.02527449  0.267206656    indus=7.07
#&gt; 4    chas -0.02598383  0.009941115        chas=0
#&gt; 5     nox  0.41512115  0.928767187     nox=0.469
#&gt; 6      rm -0.85564962 12.309151401      rm=6.421</code></pre>
<p>The <code>iml</code> package is available on <a href="https://cran.r-project.org/web/packages/iml/index.html">CRAN</a> and on <a href="https://github.com/christophM/iml">Github</a>.</p>
</div>
