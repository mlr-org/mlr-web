---
title: mlr-2.15.0
authors: ["patrick-schratz"]
date: '2019-08-06'
slug: mlr-2-15-0
bibliography: ../../static/bib/mlr-2-15-0.bib
categories:
  - R
  - r-bloggers
tags:
  - mlr
  - release
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#changes-to-benchmark">Changes to <code>benchmark()</code></a></li>
<li><a href="#changes-to-filters">Changes to Filters</a><ul>
<li><a href="#new-ensemble-filters">New ensemble filters</a></li>
<li><a href="#new-return-structure-for-filter-values">New return structure for filter values</a></li>
</ul></li>
<li><a href="#learners">Learners</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>We just released <em>mlr</em> v2.15.0 to CRAN.
This version includes some breaking changes and the usual bug fixes from the last three months.</p>
<p>We made good progress on the goal of cleaning up the <a href="https://github.com/mlr-org/mlr">Github repo</a>.
We processed nearly all open pull requests (around 40).
In the next months we will focus on cleaning up the issue tracker even though most of our time will go into improving the successor package <a href="https://github.com/mlr-org/mlr3">mlr3</a> and its extension packages.</p>
<p>Unless there are active contributions from the user side, we do not expect much feature additions for the next version(s) of <em>mlr</em>.</p>
<div id="changes-to-benchmark" class="section level1">
<h1>Changes to <code>benchmark()</code></h1>
<p>The <code>benchmark()</code> function does not store the tuning results (stored in the <code>$extract</code> slot) anymore by default.
This change was made to prevent BenchmarkResult (BMR) objects from getting huge in size (~ GB) when multiple models are compared with extensive tuning.
Unless you want to do a analysis on the tuning effects, you do not need the tuning results to compare the performance of the algorithms.
Huge BMR objects can cause various troubles.
One of them (which was the inital root for this change) appears when benchmarking is done on a HPC using multiple workers.
Each worker has a limited amount of memory and expecting a huge BMR might limit the amount of workers that can be spawned.
In addition, loading the large resulting BMR into the global environment (or merging it using <code>mergeBenchmarkResults()</code>) for post-analysis will become a pain.
To save users from all of these troubles in the first place, we decided to change the default.</p>
<p>To store the tuning results, you have to actively set <code>keep.extract = TRUE</code> from now on.
Not storing the tuning was actually already implicitly the default in <code>resample()</code> since the user had to set the <code>extract</code> argument manually to save certain results (tuning, feature importance).
With the new change the package became more consistent.</p>
</div>
<div id="changes-to-filters" class="section level1">
<h1>Changes to Filters</h1>
<div id="new-ensemble-filters" class="section level2">
<h2>New ensemble filters</h2>
<p>With this release it is possible to calculate ensemble filters with <em>mlr</em> <span class="citation">(Seijo-Pardo et al. 2017)</span>.
“Ensemble filters” are similar to ensemble models in the way that multiple filters are used to generate the ranking of features.
Multiple aggregations functions are supported (<code>min()</code>, <code>mean()</code>, <code>median()</code>, “Borda”) with the latter being the most used one in literature while writing this.</p>
<p>To our knowledge there is no other package/framework in R currently that supports ensemble filters in a similar way <em>mlr</em> does.
Since <em>mlr</em> makes it possible to use filters from a <a href="https://mlr.mlr-org.com/articles/tutorial/filter_methods.html">variety of different packages</a>, the user is able to create powerful ensemble filters.
Note however that currently you cannot tune the selection of simple filters since tuning a character vector param is not supported by <em>ParamHelpers</em>.
See <a href="https://github.com/berndbischl/ParamHelpers/pull/206">this discussion</a> for more information.</p>
<p>Here is a simple toy example how to create ensemble filters in <em>mlr</em> from <code>?filterFeatures()</code>:</p>
<pre class="r"><code>library(mlr)
## Loading required package: ParamHelpers
filterFeatures(iris.task, method = &quot;E-min&quot;,
  base.methods = c(&quot;FSelectorRcpp_gain.ratio&quot;, &quot;FSelectorRcpp_information.gain&quot;), abs = 2)
## Supervised task: iris-example
## Type: classif
## Target: Species
## Observations: 150
## Features:
##    numerics     factors     ordered functionals 
##           2           0           0           0 
## Missings: FALSE
## Has weights: FALSE
## Has blocking: FALSE
## Has coordinates: FALSE
## Classes: 3
##     setosa versicolor  virginica 
##         50         50         50 
## Positive class: NA</code></pre>
</div>
<div id="new-return-structure-for-filter-values" class="section level2">
<h2>New return structure for filter values</h2>
<p>With the added support for ensemble filters we also changes the return structure of calculated filter values.</p>
<p>The new makes it easier to apply post-analysis tasks like grouping and filtering.
The “method” of each row is now grouped into one column and the filter values are stored in a separate one.
We also added a default sorting of the results by the “value” of each “method”.</p>
<p>Below is a comparison of the old and new output:</p>
<pre class="r"><code># new
generateFilterValuesData(iris.task,
  method = c(&quot;FSelectorRcpp_gain.ratio&quot;, &quot;FSelectorRcpp_information.gain&quot;))
## FilterValues:
## Task: iris-example
##           name    type                         method     value
## 4  Petal.Width numeric       FSelectorRcpp_gain.ratio 0.8713692
## 3 Petal.Length numeric       FSelectorRcpp_gain.ratio 0.8584937
## 1 Sepal.Length numeric       FSelectorRcpp_gain.ratio 0.4196464
## 2  Sepal.Width numeric       FSelectorRcpp_gain.ratio 0.2472972
## 8  Petal.Width numeric FSelectorRcpp_information.gain 0.9554360
## 7 Petal.Length numeric FSelectorRcpp_information.gain 0.9402853
## 5 Sepal.Length numeric FSelectorRcpp_information.gain 0.4521286
## 6  Sepal.Width numeric FSelectorRcpp_information.gain 0.2672750</code></pre>
<pre class="r"><code># old
generateFilterValuesData(iris.task,
  method = c(&#39;gain.ratio&#39;,&#39;information.gain&#39;)
## FilterValues:
## Task: iris-example
##           name    type gain.ratio information.gain
## 1 Sepal.Length numeric  0.4196464        0.4521286
## 2  Sepal.Width numeric  0.2472972        0.2672750
## 3 Petal.Length numeric  0.8584937        0.9402853
## 4  Petal.Width numeric  0.8713692        0.9554360</code></pre>
</div>
</div>
<div id="learners" class="section level1">
<h1>Learners</h1>
<p>Besides the integration of new learners and some added options for integrated ones (check the <a href="https://mlr.mlr-org.com/news/index.html#learners---general">NEWS</a> file), we fixed a bug that caused an incorrect aggregation of probabilities in certain cases.
This bug was around undetected for quite some time and was revealed due to a change in <em>data.table</em>’s <code>rbindlist()</code> function.
Thankfully <a href="https://github.com/danielhorn">@danielhorn</a> reported this <a href="https://github.com/mlr-org/mlr/issues/2578">issue</a> and we could fix it within a few days.</p>
<p>Another mentionable change is that the commonly used <code>e1071::svm()</code> learner now only uses the formula interface internally if factors are present in the data.
This aims to prevent <a href="https://github.com/mlr-org/mlr/issues/1738">“stack overflow” problems</a> that some user encountered with large datasets.</p>
<p>With <a href="https://github.com/mlr-org/mlr/issues/1784">PR #1784</a> we added more support for estimating standard errors using the internal methods of the “Random Forest” algorithm.
Please check the <a href="https://mlr.mlr-org.com/news/index.html#learners---general">NEWS</a> file for more detailed information about the implemented RF learners.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-seijo-pardo2017">
<p>Seijo-Pardo, B., I. Porto-Díaz, V. Bolón-Canedo, and A. Alonso-Betanzos. 2017. “Ensemble Feature Selection: Homogeneous and Heterogeneous Approaches.” <em>Knowledge-Based Systems</em> 118 (February): 124–39. <a href="https://doi.org/10/f9qgrv">https://doi.org/10/f9qgrv</a>.</p>
</div>
</div>
</div>
