---
title: "Visualization of predictions"
authors: ["jakob-richter"]
date: 2015-07-28
categories: ["R", "r-bloggers"]
tags: ["visualization", "prediction", "rstats"]
---



<p>In this post I want to shortly introduce you to the great visualization possibilities of <code>mlr</code>.
Within the last months a lot of work has been put into that field.
This post is not a <a href="https://mlr.mlr-org.com/">tutorial</a> but more a demonstration of how little code you have to write with <code>mlr</code> to get some nice plots showing the prediction behaviors for different learners.</p>
<!--more-->
<p>First we define a list containing all the <a href="https://mlr.mlr-org.com/articles/tutorial/devel/integrated_learners.html">learners</a> we want to visualize.
Notice that most of the <code>mlr</code> methods are able to work with just the string (i.e. <code>&quot;classif.svm&quot;</code>) to know what learner you mean.
Nevertheless you can define the learner more precisely with <code>makeLearner()</code> and set some parameters such as the <code>kernel</code> in this example.</p>
<p>First we define the list of learners we want to visualize.</p>
<pre class="r"><code>library(mlr)
learners = list(
  makeLearner(&quot;classif.svm&quot;, kernel = &quot;linear&quot;),
  makeLearner(&quot;classif.svm&quot;, kernel = &quot;polynomial&quot;),
  makeLearner(&quot;classif.svm&quot;, kernel = &quot;radial&quot;),
  &quot;classif.qda&quot;,
  &quot;classif.randomForest&quot;,
  &quot;classif.knn&quot;
  )</code></pre>
<div id="support-vector-machines" class="section level2">
<h2>Support Vector Machines</h2>
<p>Now lets have a look at the different results and lets start with the SVM with a <em>linear kernel</em>.</p>
<pre class="r"><code>plotLearnerPrediction(learner = learners[[1]], task = iris.task)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/linear-svm-1.png" width="672" /></p>
<p>We can see clearly that in fact the decision boundary is indeed linear.
Furthermore the misclassified items are highlighted and a 10-fold cross validation to obtain the mean missclassification error is executed.</p>
<p>For the <em>polynomial</em> and the <em>radial kernel</em> the decision boundaries already look a bit more sophisticated:</p>
<pre class="r"><code>plotLearnerPrediction(learner = learners[[2]], task = iris.task)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/polynomial-radial-svm-1.png" width="672" /></p>
<pre class="r"><code>plotLearnerPrediction(learner = learners[[3]], task = iris.task)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/polynomial-radial-svm-2.png" width="672" /></p>
<p>Note that the intensity of the colors also indicates the certainty of the prediction and that this example is probably a rare case where the linear kernel performs best. although this is likely only the case because we didn’t optimize the parameters for the radial kernel.</p>
</div>
<div id="quadratic-discriminant-analysis" class="section level2">
<h2>Quadratic Discriminant Analysis</h2>
<pre class="r"><code>plotLearnerPrediction(learner = learners[[4]], task = iris.task)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/qda-1.png" width="672" /></p>
<p>A well known classificator from the basic course of statistics delivers a similar performance as the SVMs.</p>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<pre class="r"><code>plotLearnerPrediction(learner = learners[[5]], task = iris.task)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/randomforest-1.png" width="672" /></p>
<p>A completely different picture is generated by the random forest.
Here you can see that the whole data set is used to generate the model and as a result it looks like it gives a perfect fit but obviously you wouldn’t use the train data to evaluate your model.
And the results of the 10-fold cross validation indicate that the random forest is actually not better then the others.</p>
</div>
<div id="nearest-neighbour" class="section level2">
<h2>Nearest Neighbour</h2>
<pre class="r"><code>plotLearnerPrediction(learner = learners[[6]], task = iris.task)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/knn-1.png" width="672" /></p>
<p>In the default setting knn just look for ‘k=1’ neighbor and as a result the classifier does not return probabilities but only the class labels.</p>
</div>
